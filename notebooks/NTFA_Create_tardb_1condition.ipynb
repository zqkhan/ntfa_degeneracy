{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8ddb76a-5128-4dfc-a413-33c6a008a479",
   "metadata": {},
   "source": [
    "# Create tardb for AVFP fear runs\n",
    "\n",
    "creating tardb for AVFP for one condition\n",
    "\n",
    "2022 June\n",
    "Yiyu\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "716e1e24-f4bf-413d-ba2f-4d8c342f3ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# conda environments:\n",
      "#\n",
      "HTFATorch                /home/wang.yiyu/.conda/envs/HTFATorch\n",
      "NTFA_env3             *  /home/wang.yiyu/.conda/envs/NTFA_env3\n",
      "base                     /shared/centos7/anaconda3/3.7\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# check we're in our env (*)\n",
    "%conda env list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31e44786-268f-43f8-af5f-f0dbad352918",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wang.yiyu/.conda/envs/NTFA_env3/lib/python3.6/site-packages/nilearn/datasets/__init__.py:96: FutureWarning: Fetchers from the nilearn.datasets module will be updated in version 0.9 to return python strings instead of bytes and Pandas dataframes instead of Numpy arrays.\n",
      "  \"Numpy arrays.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# path for the NTFA package\n",
    "NTFA_path = \"/work/abslab/NTFA_packages/NTFADegeneracy/\"\n",
    "\n",
    "import sys\n",
    "sys.path.append(NTFA_path)\n",
    "import htfa_torch.niidb as niidb\n",
    "import htfa_torch.utils as utils\n",
    "import htfa_torch.tardb as tardb\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import webdataset as wds\n",
    "import torch\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s %(message)s', datefmt='%m/%d/%Y %H:%M:%S',\n",
    "                    level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f900aa9-c9f5-44bf-875f-a3784e7745b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nifti_dir = '/work/abslab/AVFP/denoised/'\n",
    "logfiles_dir = '/work/abslab/AVFP/logfiles/AffVidsNovel_logfiles/'\n",
    "\n",
    "mask_dir = '/home/wang.yiyu/AVFP/masks/'\n",
    "base_dir = '/work/abslab/Yiyu/NTFA_AVFP/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77ebff2e-36ce-4cc9-8711-5c9228a3f889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters:\n",
    "\n",
    "TASK_ONSET_DELAY = 5 # HRF-lag delay in seconds to add to onset/offset\n",
    "\n",
    "TASK_TR = 0.8 #seconds per TR\n",
    "\n",
    "exclude_tasks = ['rest','Social','Spiders'] #used for z-scoring, but don't want to model as a condition\n",
    "\n",
    "\n",
    "# define a subset of participants to analyze:\n",
    "# if all, set subs to 'All'\n",
    "# if a subset, provide the number (int) of participants to be included\n",
    "subs = 20\n",
    "\n",
    "# using GM (and SNR) or SNR only?\n",
    "mask_type = 'GM' #'GMandSNR' #'GM', 'SNR'\n",
    "\n",
    "\n",
    "# define condition\n",
    "condition = 'HeightsOnly'\n",
    "\n",
    "\n",
    "log_file_headers = np.array(['video_name','video_number','video_category','novel_vs_familiar',\n",
    "                   'run_number','video_onset','video_offset','duration_method1','duration_method2',\n",
    "                   'fear_rating_onset','fear_rating','fear_rating_RT',\n",
    "                   'arousal_rating_onset','arousal_rating','arousal_rating_RT',\n",
    "                   'valence_rating_onset','valence_rating','valence_rating_RT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f767394b-b18c-4c39-ad2c-2f0cd433bed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "example file name: /work/abslab/AVFP/denoised/110/sub-110_run-3_AVFP_denoised_novideoregs.nii.gz \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# function to getch nifti files per subject and run:\n",
    "\n",
    "\n",
    "def nifti_filename(subject, run):\n",
    "    AVFP_FILENAME_TEMPLATE = f'{subject}/sub-{subject}_run-{run}_AVFP_denoised_novideoregs.nii.gz'\n",
    "    return nifti_dir + AVFP_FILENAME_TEMPLATE\n",
    "\n",
    "# check:\n",
    "print('\\nexample file name:',nifti_filename(110,3),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "874e7be4-180a-4bb8-a836-db54a052dcc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['100', '103', '104', '105', '106', '107', '108', '109', '111', '112', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '127', '128', '130', '131', '132', '134', '135', '136', '137', '138', '139', '140', '142', '143', '144', '145', '146', '149', '150', '151', '152', '153', '154', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '169', '170', '171', '172', '174', '175', '176', '177', '179', '181', '182', '183', '184', '185', '186']\n"
     ]
    }
   ],
   "source": [
    "# load list of subjects included in our analyses:\n",
    "included_data = pd.read_csv(base_dir + 'fmri_info/included_avfp_subjects.csv', header=None)\n",
    "subIDs = included_data[0].astype('str').tolist()\n",
    "print(subIDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b838dd96-d322-43dc-a100-d94ba529177f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(subIDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8140ae4-7fe4-4657-b12d-c7e69a4bea50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "using 20 logfiles out of 71 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# behavioural log files, containing one row per trial\n",
    "files = glob.glob(logfiles_dir + '*.txt')\n",
    "files = [[i for i in files if 'sub_' + s in i] for s in subIDs]\n",
    "n_files = len(files)\n",
    "\n",
    "if subs != 'All': #if using a subset\n",
    "    log_files = files[:subs]\n",
    "else: #use all files\n",
    "    log_files = files[:]\n",
    "print('\\nusing',len(log_files),'logfiles out of',n_files,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a041c895-39ab-4656-81b9-5cd13eb0019b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask_file = mask_dir + 'NTFA/masks/' + mask_type + '_memorygroup_mask_N' + str(n_files) + '_downsampled.nii.gz'\n",
    "mask_file = mask_dir + 'gm_mask_icbm152_brain.nii.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd09f9b1-564e-4040-b1a5-ac629b8b0a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving database to: /work/abslab/Yiyu/NTFA_AVFP/data/downsampled_test/AVFP_NTFA_N71_subsetN20_GMmask.tar\n"
     ]
    }
   ],
   "source": [
    "# define database filename\n",
    "if subs != 'All':\n",
    "    #AVFP_FILE = 'data/AVFP_NTFA_memory_N' + str(n_files) + '_subsetN' + str(subs) + '_' + mask_type + 'mask.tar'\n",
    "    AVFP_FILE = base_dir + f'data/downsampled_test/AVFP_NTFA_N{n_files}_subsetN{subs}_{mask_type}mask_{condition}.tar'\n",
    "else: #including all subjects\n",
    "    AVFP_FILE = base_dir + f'data/AVFP_NTFA_N{n_files}_{subs}_{mask_type}mask_{condition}.tar'\n",
    "print('\\nSaving database to:',AVFP_FILE)\n",
    "\n",
    "tar_file = AVFP_FILE\n",
    "sink = wds.TarWriter(tar_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4ed88a3-bfdf-4741-8a8f-ae0b6cb2260f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'html_name' (str)\n"
     ]
    }
   ],
   "source": [
    "html_name = AVFP_FILE.split('.')[0] + '.html'\n",
    "%store html_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d55438c-d1d4-4534-9163-45ec4c226755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/work/abslab/Yiyu/NTFA_AVFP/data/downsampled_test/AVFP_NTFA_N71_subsetN20_GMmask'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AVFP_FILE.split('.')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51bc850-d3a6-4a33-842b-7b5d2bad2a77",
   "metadata": {},
   "source": [
    "# Functions to create labels in database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "853f7ebb-a947-46bf-952a-fd51951a2ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up functions to read task files:\n",
    "class TaskElement:\n",
    "    def __init__(self, task, start, end, run, fear_rating=None):\n",
    "        def round_off_time(t):\n",
    "            if t is not None:\n",
    "                if task != 'rest':\n",
    "                    return round((t + TASK_ONSET_DELAY) / TASK_TR) #assumes task onsets are in seconds\n",
    "                else:\n",
    "                    return round(t)\n",
    "            else:\n",
    "                return None\n",
    "        self.task = task\n",
    "        self.start_time = round_off_time(start)\n",
    "        self.end_time = round_off_time(end)\n",
    "        self.run = run\n",
    "        self.fear_rating = fear_rating\n",
    "\n",
    "        \n",
    "def parse_task_lines(lines, headers):\n",
    "    study = 'AVFP'\n",
    "    for (i, line) in enumerate(lines):\n",
    "        cols = line.split(' ')\n",
    "        task = cols[int(np.where(headers == 'video_name')[0])]\n",
    "        mem = cols[int(np.where(log_file_headers == 'novel_vs_familiar')[0])]\n",
    "        if mem == '1':\n",
    "            mem_type = 'New'\n",
    "        else: mem_type = 'Old'\n",
    "        task = f'{mem_type}_{task[:-4]}_{study}'\n",
    "        \n",
    "        start_time = float(cols[int(np.where(headers == 'video_onset')[0])])\n",
    "        end_time = float(cols[int(np.where(headers == 'video_offset')[0])])\n",
    "        run = int(cols[int(np.where(headers == 'run_number')[0])])\n",
    "        fear_rating = abs(float(cols[int(np.where(headers == 'fear_rating')[0])]))\n",
    "        if np.isnan(fear_rating): # if didn't move the slider at all\n",
    "            fear_rating = .5 #middle\n",
    "        yield TaskElement(task, start_time, end_time, run, fear_rating)\n",
    "\n",
    "        \n",
    "def rest_tasks(tasks):\n",
    "    yield TaskElement('rest', 0, tasks[0].start_time - 1, tasks[0].run)\n",
    "    for i in range(1, len(tasks)):\n",
    "        rest_start = tasks[i-1].end_time + 1\n",
    "        rest_end = tasks[i].start_time - 1\n",
    "        if tasks[i].run == tasks[i-1].run:\n",
    "            yield TaskElement('rest', rest_start, rest_end, tasks[i].run)\n",
    "        else:\n",
    "            yield TaskElement('rest', rest_start, None, tasks[i-1].run)\n",
    "            yield TaskElement('rest', 0, rest_end, tasks[i].run)\n",
    "    yield TaskElement('rest', tasks[-1].end_time + 1, None, tasks[-1].run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0be96d98-542e-419b-8ce6-4328817bf5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tasks(task_csv, headers):\n",
    "    def sentinel(f):\n",
    "        return f if f is not None else 0.0\n",
    "    subject = int(task_csv.split('.txt')[0][-3:])\n",
    "    logging.info('Subject %d', subject)   \n",
    "    \n",
    "    with open(task_csv, 'r') as task_csv_file:\n",
    "        \n",
    "        \n",
    "        \n",
    "        task_lines = list(parse_task_lines(task_csv_file.readlines(), headers))        \n",
    "        task_lines += list(rest_tasks(task_lines))\n",
    "        rest_lines = [r for r in task_lines if r.task == 'rest']\n",
    "        rest_lines = sorted(rest_lines, key=lambda t: sentinel(t.run))\n",
    "        rest_starts_dict = {key: [] for key in range(3, 6)} # runs 3,4,5 of AVFP\n",
    "        rest_ends_dict = {key: [] for key in range(3, 6)}\n",
    "        for (i,rest) in enumerate(rest_lines):\n",
    "            if rest.end_time is not None and rest.start_time is not None:\n",
    "                rest_ends_dict[rest.run].append(rest.end_time)\n",
    "                rest_starts_dict[rest.run].append(rest.start_time)\n",
    "        task_lines = sorted(task_lines, key=lambda t: sentinel(t.start_time))\n",
    "        for (i, task) in enumerate(task_lines):\n",
    "            if any(ele in task.task for ele in exclude_tasks):\n",
    "                continue\n",
    "            logging.info('Saving %s, run %d: started at %f, ended at %f',\n",
    "                         task.task, task.run, sentinel(task.start_time), sentinel(task.end_time))\n",
    "            result = niidb.FMriActivationBlock(zscore=True, zscore_by_rest=True)\n",
    "            result.subject = subject\n",
    "            result.task = task.task\n",
    "            result.run = task.run\n",
    "            result.start_time = task.start_time\n",
    "            result.end_time = task.end_time\n",
    "            result.rest_start_times = rest_starts_dict[result.run]\n",
    "            result.rest_end_times = rest_ends_dict[result.run]\n",
    "            result.individual_differences = {'fear_rating': task.fear_rating}\n",
    "            yield result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dcc2f3-23b1-47e1-a8f4-a7fb2a35f98c",
   "metadata": {},
   "source": [
    "# Create database!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b42d0f-9aa2-43a5-9c8f-d7fb656bfc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the database:\n",
    "# note that the other blocks (rest) include all TRs between the videos\n",
    "\n",
    "total_trs = 0\n",
    "metadata = {\n",
    "    'blocks': []\n",
    "}\n",
    "block_id = 0\n",
    "for text_file in log_files:\n",
    "    for block in read_tasks(text_file[0], log_file_headers):\n",
    "        block.filename = nifti_filename(block.subject, block.run)\n",
    "        block.rest_end_times = '[' + ', '.join(map(str, block.rest_end_times)) + ']'\n",
    "        block.rest_start_times = '[' + ', '.join(map(str, block.rest_start_times)) + ']'\n",
    "        block.block = block_id\n",
    "        block_id += 1\n",
    "        block.mask = mask_file\n",
    "        block.smooth = 6\n",
    "        block.load()\n",
    "        metadata['blocks'].append(block.wds_metadata())\n",
    "\n",
    "        for vals in block.format_wds():\n",
    "            sink.write(vals)\n",
    "        block_trs = (block.end_time - block.start_time)\n",
    "        total_trs += block_trs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a574a4d-6e48-4a64-a0e3-b617a68132a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata['voxel_locations'] = block.locations\n",
    "metadata['num_times'] = total_trs\n",
    "torch.save(metadata, tar_file + '.meta')\n",
    "logging.info('Recorded metadata, including voxel locations')\n",
    "sink.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8396536b-ba68-49b5-afd4-6dc55e569702",
   "metadata": {},
   "outputs": [],
   "source": [
    "avfp_db = tardb.FmriTarDataset(AVFP_FILE)\n",
    "avfp_db.mean_block(save=True)\n",
    "avfp_db.normalize_activations(save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bcdf2a-395c-4193-a134-19caca0a8c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info('Finished building TarDb out of AVFP runs 3, 4, 5 dataset in %s',AVFP_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258871a2-8184-4265-adae-5d3b32fa2376",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ff3c06-d053-4e36-98d2-af3570044b98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78aa6a2b-ee47-494c-a3e4-bb461f632a79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c950ce5e-e0db-4c04-8018-803c52040fb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
