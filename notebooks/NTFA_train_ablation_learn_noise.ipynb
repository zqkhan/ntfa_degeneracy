{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f682fb80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# conda environments:\n",
      "#\n",
      "HTFATorch                /home/wang.yiyu/.conda/envs/HTFATorch\n",
      "NTFA_env3             *  /home/wang.yiyu/.conda/envs/NTFA_env3\n",
      "base                     /shared/centos7/anaconda3/3.7\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# check we're in our env (*)\n",
    "%conda env list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1829c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using NTFA code from: /work/abslab/Yiyu/NTFA_packages/ntfa_degeneracy/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wang.yiyu/.conda/envs/NTFA_env3/lib/python3.6/site-packages/nilearn/datasets/__init__.py:96: FutureWarning: Fetchers from the nilearn.datasets module will be updated in version 0.9 to return python strings instead of bytes and Pandas dataframes instead of Numpy arrays.\n",
      "  \"Numpy arrays.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# path for the NTFA package\n",
    "#NTFA_path = \"/work/abslab/NTFA_packages/NTFADegeneracy/\" #combination embedding NTFA\n",
    "#NTFA_path = \"/work/abslab/NTFA_packages/NTFATorch/\" #base NTFA (just P and S)\n",
    "NTFA_path = \"/work/abslab/Yiyu/NTFA_packages/ntfa_degeneracy/\"\n",
    "print(\"Using NTFA code from:\", NTFA_path)\n",
    "\n",
    "import sys\n",
    "sys.path.append(NTFA_path)\n",
    "import htfa_torch.dtfa as DTFA\n",
    "import htfa_torch.niidb as niidb\n",
    "import htfa_torch.tardb as tardb\n",
    "import htfa_torch.utils as utils\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import webdataset as wds\n",
    "\n",
    "import torch\n",
    "import itertools\n",
    "import timeout_decorator\n",
    "import matplotlib.pyplot as plt\n",
    "from ordered_set import OrderedSet\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s %(message)s', datefmt='%m/%d/%Y %H:%M:%S',\n",
    "                    level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7431126",
   "metadata": {},
   "outputs": [],
   "source": [
    "nifti_dir = '/work/abslab/AVFP/denoised/'\n",
    "logfiles_dir = '/work/abslab/AVFP/logfiles/AffVidsNovel_logfiles/'\n",
    "\n",
    "mask_dir = '/home/wang.yiyu/AVFP/masks/'\n",
    "base_dir = '/work/abslab/Yiyu/NTFA_AVFP/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4793fbd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['100', '103', '104', '105', '106', '107', '108', '109', '111', '112', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '127', '128', '130', '131', '132', '134', '135', '136', '137', '138', '139', '140', '142', '143', '144', '145', '146', '149', '150', '151', '152', '153', '154', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '169', '170', '171', '172', '174', '175', '176', '177', '179', '181', '182', '183', '184', '185', '186']\n",
      "total subs = 71\n",
      "setting voxel noise to 0.8\n",
      "noise option: fixed-0.8\n"
     ]
    }
   ],
   "source": [
    "# **** parameters that define the model directory ******\n",
    "subs = 20 #'All' #note, database file must have been created already for these subjects\n",
    "\n",
    "\n",
    "included_data = pd.read_csv(base_dir + 'fmri_info/included_avfp_subjects.csv', header=None)\n",
    "subIDs = included_data[0].astype('str').tolist()\n",
    "print(subIDs)\n",
    "total_subs = len(subIDs)\n",
    "print(f\"total subs = {total_subs}\")\n",
    "\n",
    "# using GM (and SNR) or SNR only?\n",
    "mask_type = 'GM' #'GMandSNR' #'SNR' or 'GM'\n",
    "\n",
    "# penalty weights (participant, stimulus, combination)\n",
    "p_weight, s_weight, c_weight = 1, 1, 1\n",
    "tuning = False    # if True, it uses the existing baseline model to further train\n",
    "# while *keeping the factor weights consistent*\n",
    "\n",
    "# which embedding-to-weight mappings should be linear?\n",
    "linear_opts = 'None' #'Base' 'None' 'C'\n",
    "# train based on a penalty weight combination\n",
    "# ******************************************************\n",
    "\n",
    "# additional parameters:\n",
    "n_epoch = 1000\n",
    "n_factor = 100\n",
    "n_check = 50 # save checkpoints every n_check epochs for model\n",
    "\n",
    "# learning rates (**change/check if re-running based on latest html**)\n",
    "lr_q = 1e-2 #default = 1e-2\n",
    "lr_p = 1e-4  #default = 1e-4\n",
    "\n",
    "# condition\n",
    "condition = '_HeightsOnly'\n",
    "\n",
    "# noise \n",
    "learn_noise = True\n",
    "set_noise = True\n",
    "if set_noise:\n",
    "    voxel_noise = 0.3 # run the check_voxel_noise.ipynb before to get a sense\n",
    "    print(f\"setting voxel noise to {voxel_noise}\")\n",
    "else:        \n",
    "    voxel_noise = 0.1 #default\n",
    "          \n",
    "if learn_noise:\n",
    "    noise_opts = f'learned-{voxel_noise}' \n",
    "else:\n",
    "    noise_opts = f'fixed-{voxel_noise}'\n",
    "print(f'noise option: {noise_opts}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7911fa30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tesla V100-SXM2-32GB'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.set_device(0)\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ef449a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching database: /work/abslab/Yiyu/NTFA_AVFP/data/downsampled_test/AVFP_NTFA_N71_subsetN20_GMmask_HeightsOnly.tar\n"
     ]
    }
   ],
   "source": [
    "# define database filename\n",
    "if subs != 'All':\n",
    "    #AVFP_FILE = 'data/AVFP_NTFA_memory_N' + str(n_files) + '_subsetN' + str(subs) + '_' + mask_type + 'mask.tar'\n",
    "    AVFP_FILE = base_dir + f'data/downsampled_test/AVFP_NTFA_N{total_subs}_subsetN{subs}_{mask_type}mask{condition}.tar'\n",
    "else: #including all subjects\n",
    "    AVFP_FILE = base_dir + f'data/AVFP_NTFA_N{total_subs}_{subs}_{mask_type}mask{condition}.tar'\n",
    "print('\\nFetching database:',AVFP_FILE)\n",
    "\n",
    "avfp_db = tardb.FmriTarDataset(AVFP_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ae80481-a559-4124-ae83-4022957e7ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do we want to hold out blocks to test generalization? If so, how many?\n",
    "hold_out_data = True\n",
    "n_per_subj = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c667fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excluding 40 blocks from training\n",
      "IDs: [2, 7, 13, 14, 24, 34, 43, 46, 49, 57, 60, 61, 81, 83, 84, 94, 102, 107, 109, 110, 123, 130, 142, 143, 144, 155, 156, 166, 168, 174, 187, 190, 195, 199, 204, 215, 217, 225, 228, 239]\n"
     ]
    }
   ],
   "source": [
    "# specify training and testing sample:\n",
    "\n",
    "if hold_out_data:\n",
    "    rng = np.random.default_rng(2022)\n",
    "    test_blocks = []\n",
    "    for p in avfp_db.subjects():\n",
    "        sub_tasks = [b['task'] for b in avfp_db.blocks.values() if b['subject'] == p]\n",
    "        idx = rng.choice(len(sub_tasks), n_per_subj, replace=False)\n",
    "        for i in idx:\n",
    "            test_blocks.extend([b['id'] for b in avfp_db.blocks.values() if (b['subject'] == p) & (b['task'] == sub_tasks[i])])\n",
    "    test_blocks = np.sort(test_blocks).tolist()\n",
    "    print('Excluding',len(test_blocks),'blocks from training\\nIDs:',test_blocks)  \n",
    "else:\n",
    "    test_blocks = []\n",
    "    print('Including all blocks in training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e38e0ed",
   "metadata": {},
   "source": [
    "# Run pass information to DeepTFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "622471e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "saving model to:  models/ablation_comparison/AVFP_NTFA_sub-20_epoch-1000_factor-100_mask-GM_111_lin-PSC_noise-fixed-0.8/ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# set up directory and filename for saving model\n",
    "query_dir = f'models/ablation_comparison/AVFP_NTFA_sub-{subs}_epoch-{n_epoch}_factor-{n_factor}_mask-{mask_type}_{p_weight}{s_weight}{c_weight}_lin-{linear_opts}_noise-{noise_opts}/'\n",
    "\n",
    "if not os.path.isdir(query_dir):\n",
    "    os.makedirs(query_dir)\n",
    "print(\"\\nsaving model to: \", query_dir,'\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "251a0684",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dtfa = DTFA.DeepTFA(avfp_db, num_factors=n_factor, linear_params=linear_opts, query_name=query_dir, voxel_noise=voxel_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "097e251f-8cd7-44e6-be84-a24dc69eae4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8000])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtfa.generative.hyperparams.voxel_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91bc4dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of voxels: 191002\n"
     ]
    }
   ],
   "source": [
    "print('number of voxels:',dtfa.num_voxels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18fbacda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of trials: 240\n"
     ]
    }
   ],
   "source": [
    "print('number of trials:',dtfa.num_blocks) # should be N subjects * 36 videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f05262c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique tasks: 12\n"
     ]
    }
   ],
   "source": [
    "print('number of unique tasks:',len(dtfa.tasks()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0decb8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subjects analyzed: [100, 103, 104, 105, 106, 107, 108, 109, 111, 112, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123]\n"
     ]
    }
   ],
   "source": [
    "print('subjects analyzed:',dtfa.subjects())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e0b2dcd-f854-4af3-b486-171ad531f1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subjects analyzed: [100, 103, 104, 105, 106, 107, 108, 109, 111, 112, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123]\n"
     ]
    }
   ],
   "source": [
    "print('subjects analyzed:',dtfa.subjects())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82055d26",
   "metadata": {},
   "source": [
    "# Train the model!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c063151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No checkpoint found — starting training\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# name of the most recent model checkpoint, if generated (prefix for .dtfa_model and .dtfa_guide)\n",
    "checkpoint_files = glob.glob(query_dir + 'CHECK*.dtfa*')\n",
    "\n",
    "if not tuning:\n",
    "    if len(checkpoint_files) > 0:\n",
    "        state_name = max(checkpoint_files, key=os.path.getctime).split('.dtfa')[0]\n",
    "        n_checkpoint_epochs = int(state_name.split('Epoch')[1].split('.')[0])\n",
    "        print('\\nRestarting from most recent checkpoint:',state_name,'\\n')\n",
    "        dtfa.load_state(state_name)\n",
    "    else:\n",
    "        n_checkpoint_epochs=0\n",
    "        print('\\nNo checkpoint found — starting training\\n')\n",
    "    \n",
    "# if starting tuning, load in the baseline model:\n",
    "elif tuning:\n",
    "    baseline_dir = query_dir.split('mask_')[0] + 'mask_111/'\n",
    "    baseline_files = glob.glob(baseline_dir + 'CHECK*.dtfa*')\n",
    "    if len(baseline_files) > 0:\n",
    "        baseline_name = max(baseline_files, key=os.path.getctime).split('.dtfa')[0]\n",
    "        n_checkpoint_epochs = int(baseline_name.split('Epoch')[1].split('.')[0])\n",
    "        print('\\nStarting from baseline model:',baseline_name,'\\n')\n",
    "        dtfa.load_state(baseline_name)\n",
    "    else:\n",
    "        raise Exception('No baseline model found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae141731",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# html_files = glob.glob(query_dir + '*train*.html')\n",
    "# if len(html_files) > 0:\n",
    "#     html_name = max(html_files, key=os.path.getctime)\n",
    "#     f=open(html_name,'r')\n",
    "#     html_data=f.read()\n",
    "#     if 'reducing learning rate' in html_data:\n",
    "#         lr_q = float(re.findall('learning rate of group 2 to \\d+.\\d+e-\\d+',data)[-1].split('to ')[-1])\n",
    "#         lr_p = float(re.findall('learning rate of group 3 to \\d+.\\d+e-\\d+',data)[-1].split('to ')[-1])\n",
    "#         print('\\nUsing updated learning rate:',lr_q, lr_p)\n",
    "#     else: print('\\nUsing default learning rate:',lr_q, lr_p)\n",
    "# else: \n",
    "#     print('\\nStarting with default learning rate:',lr_q, lr_p) \n",
    "\n",
    "\n",
    "# it seems even if the learning rate didn't change in the training, there is still a jump in the loss whenever the training starts from the check point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4dd86d3d-32f1-402a-b66d-6d6aed2965ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeout_decorator.timeout(25200) # 7 hours\n",
    "def train_ntfa():\n",
    "    dtfa.train(num_steps=n_epoch-n_checkpoint_epochs, num_steps_exist=n_checkpoint_epochs,\n",
    "               learning_rate={'q': lr_q, 'p': lr_p}, log_level=logging.INFO, num_particles=1,\n",
    "               batch_size=128, use_cuda=True, checkpoint_steps=n_check, patience=50,\n",
    "               blocks_filter=avfp_db.inference_filter_blocks(training=True, exclude_blocks=test_blocks),learn_voxel_noise=learn_noise\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f99758d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print('training')\n",
    "    train_ntfa()\n",
    "except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466cca38-03d6-49e3-a172-3d7126ccee91",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtfa.generative.hyperparams.voxel_noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6152a3",
   "metadata": {},
   "source": [
    "# plot loss after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9548bfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in all saved losses txt files\n",
    "loss_files = np.sort(glob.glob(query_dir + '*_losses.txt'))\n",
    "losses = []\n",
    "for i in loss_files:\n",
    "    losses.append(np.loadtxt(i)) \n",
    "losses = np.concatenate(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a18efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adpated from function in utils\n",
    "def plot_losses(losses):\n",
    "    epochs = range(losses.shape[0])\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, losses, '-', lw=3)\n",
    "    plt.title('Free-energy / -ELBO change over training', fontsize=20)\n",
    "    plt.xlabel('Epoch', fontsize=16)\n",
    "    plt.ylabel('Free-energy / -ELBO (nats)', fontsize=16)\n",
    "    plt.savefig(query_dir + 'losses.pdf')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e566fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loss function over',len(losses),'epochs\\n')\n",
    "plot_losses(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cce4ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e460ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_name = query_dir + 'NTFA_AVFP_train_Epoch' + str(len(losses)) + '.html'\n",
    "%store html_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24614712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook_name = 'NTFA_train_ablation.ipynb'\n",
    "# os.system('jupyter nbconvert ' + notebook_name + ' --to html --output ' + html_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86f674a-178d-4ee1-a933-db3fefd0a6bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
